---
title: "week12notes"
format: html 
---

```{R}
packages <- c(
  "dplyr", 
  "readr", 
  "tidyr", 
  "purrr", 
  "stringr", 
  "corrplot", 
  "car", 
  "caret", 
  "torch", 
  "nnet", 
  "broom",
  "torch",
  "torchvision",
  "e1071",
  "glmnet",
  "nnet",
  "rpart",
  "ISLR2",
  'luz',
  'torchvision'
)

renv::install(packages)
sapply(packages, require, character.only=T)
```



## Tuesday April 4th

```{r}
ex <- \(x) ifelse(
  ((abs(x[1]) + 0.05 * rnorm(1) > 0.50 & abs(x[2]) + 0.05 * rnorm(1) > 0.50)) |
  ((abs(x[1]) + 0.05 * rnorm(1) < 0.25 & abs(x[2]) + 0.05 * rnorm(1) < 0.25)),1,0  
)

n <- 300
X <- t(replicate(n, 2 * runif(2) - 1))
y <- apply(X, 1, ex) %>% as.factor()
col <- ifelse(y == 0, 'blue', 'red')
df <- data.frame(y = y, x1 = X[,1], x2 = X[,2])
plot(df$x1, df$x2, col = col, pch = 19)

Xnew <- cbind(
  rep(seq(-1.1, 1.1, length.out = 50), 50),
  rep(seq(-1.1, 1.1, length.out = 50), each = 50)
)

df_new = data.frame(x1 = Xnew[,1], x2 = Xnew[,2])
```


```{r}
Xnew <- cbind(
  rep(seq(-1.1, 1.1, length.out = 50), 50),
  rep(seq(-1.1, 1.1, length.out = 50), each = 50)
)

df_new = data.frame(x1=Xnew[,1], x2=Xnew[,2])

plt <- function(f,x){
  plot(x[,1], x[,2], col=ifelse(f(x) < 0.5, 'blue', 'red'), pch = 22)
  points(df$x1, df$x2, col = ifelse(y == '0', 'blue', 'red'), pch = 19)
}

overview <- function(f){
  predicted <- ifelse(f(df[,1]) < 0.5, 1, 0)
  actual <- df[,1]
  table(predicted, actual)
}
```


#### Neural Network with 1 hidden layer

```{r}
p <- 2
q <- 20

hh1_module <- nn_module(
  initialize = function() {
    self$f <- nn_linear(p,q)
    self$g <- nn_linear(q,1)
    self$a <- nn_relu()
    self$s <- nn_sigmoid()
  },
  forward = function(x) {
    x %>%
      self$f() %>%
      self$a() %>%
      self$g() %>%
      self$s()
  }
)
```


#### Neural Network with 2 hidden layers

```{r}
p <- 2
q1 <- 100
q2 <- 20

hh2_module <- nn_module(
  initialize = function() {
    self$f <- nn_linear(p,q1)
    self$g <- nn_linear(q1,q2)
    self$h <- nn_linear(q2,1)
    self$a <- nn_relu()
    self$s <- nn_sigmoid()
  },
  forward = function(x) {
    x %>%
      self$f() %>%
      self$a() %>%
      self$g() %>%
      self$a() %>%
      self$h() %>%
      self$s()
  }
)

```


```{r}
classifier <- 
  function(train, type = 'nn', ...){
    if(type == 'logistic'){
      f = \(x) glm(y ~ x1 + x2, train, family = binomial()) %>%
        predict(., x, type = 'response')
    }
    else if(type == 'rpart'){
      f = \(x)
        rpart(y ~ x1 + x2, df, method = 'class') %>%
        predict(., x, type = 'class') %>%
        as.numeric(.) - 1
    }
    else if(type == 'svm'){
      f = \(x)
          svm(y ~ x1 + x2, df, kernel = 'radial') %>%
            predict(., x) %>%
            as.numeric(.) - 1
    }
    else if(type == 'nn'){
      X_tensor <- torch_tensor(train[,-1] %>% as.matrix(), dtype = torch_float())
      y_tensor <- torch_tensor(cbind(train$y %>% as.numeric() - 1), dtype = torch_float())
      F <- hh2_module()
      optimizer <- optim_adam(F$parameters, lr = 0.05)
      epochs <- 1000
      
      for (i in 1:epochs){
        loss <- nn_bce_loss()(F(X_tensor), y_tensor)
        optimizer$zero_grad()
        loss$backward()
        optimizer$step()
      }
      f = \(x) as_array(F( torch_tensor(x %>% as.matrix(), dtype = torch_float()) ))
    }
    return(f)
  }

```


```{r}
f <- classifier(df, 'logistic')
plt(f, df_new)
#overview(df)
```

```{r}
f <- classifier(df, 'rpart')
plt(f, df_new)
#overview(df)
```

```{r}
f <- classifier(df, 'svm')
plt(f, df_new)
#overview(df)
```
```{r}
f <- classifier(df, 'nn')
plt(f, df_new)
#overview(df)
```


```{r}
F <- hh1_module()
F( torch_randn(20,2) )
# output would be 20 by 1 tensor, that contains the sigmoid probabilities

# if get rid of activation layer (reLU), output doesn't change since activation is only happening internally (will still see probability values)

# if get rid of sigmoid layer, will still by 20 by 1 tensor that instead shows actual values and not probabilities (may have negative numbers)


F <- hh2_module()
F( torch_randn(20,2) )
```



Exponentially increasing sin wave
```{r}
generate_data <- function(n, noise = 0.1) {
  x <- seq(1*pi, 1.7*pi, length.out = n)
  y <- exp(x) * (sin(150/x) + rnorm(n, 0, noise))
  data.frame(x = x, y = y)
}

df <- generate_data(200, noise = 0.1)
plot(df$x, df$y, pch=19)
```

```{r}
plt_reg <- function (f, x){
  ynew <- f(x)
  ylim <- range(c(ynew, df$y))
  ylim[1] <- max(c(-800, ylim[1]))
  ylim[2] <- min(c(250, ylim[2]))
  xlim <-range(x)
  plot(df$x, df$y, pch = 22, col = 'red', xlim=xlim, ylim = ylim)
  points(x[,1], ynew, pch=22, type='l')
}  
```


```{r}
#regressor formula is same as classifier except it uses nn_mse_loss() instead of nn_bce_loss()

regressor <- 
  function(train, type = 'nn', ...){
    if(type == 'logistic'){
      f = \(x) glm(y ~ x1 + x2, train, family = binomial()) %>%
        predict(., x, type = 'response')
    }
    else if(type == 'rpart'){
      f = \(x)
        rpart(y ~ x1 + x2, df, method = 'class') %>%
        predict(., x, type = 'class') %>%
        as.numeric(.) - 1
    }
    else if(type == 'svm'){
      f = \(x)
          svm(y ~ x1 + x2, df, kernel = 'radial') %>%
            predict(., x) %>%
            as.numeric(.) - 1
    }
    else if(type == 'nn'){
      X_tensor <- torch_tensor(train[,-1] %>% as.matrix(), dtype = torch_float())
      y_tensor <- torch_tensor(cbind(train$y %>% as.numeric() - 1), dtype = torch_float())
      F <- hh2_module()
      optimizer <- optim_adam(F$parameters, lr = 0.05)
      epochs <- 1000
      
      for (i in 1:epochs){
        loss <- nn_mse_loss()(F(X_tensor), y_tensor)
        optimizer$zero_grad()
        loss$backward()
        optimizer$step()
      }
      f = \(x) as_array(F( torch_tensor(x %>% as.matrix(), dtype = torch_float()) ))
    }
    return(f)
  }

# can change learning rate to mess around with output to get different results
```

```{r}
Xnew <- cbind(
  rep(seq(-1.1, 1.1, length.out = 50), 50),
  rep(seq(-1.1, 1.1, length.out = 50), each = 50)
)

df_new = data.frame(x1=Xnew[,1], x2=Xnew[,2])

plt <- function(f,x){
  plot(x[,1], x[,2], col=ifelse(f(x) < 0.5, 'blue', 'red'), pch = 22)
  points(df$x1, df$x2, col = ifelse(y == '0', 'blue', 'red'), pch = 19)
}

overview <- function(f){
  predicted <- ifelse(f(df[,1]) < 0.5, 1, 0)
  actual <- df[,1]
  table(predicted, actual)
}
```

```{r}
f <- regressor(df, 'svm')
plt(f, df_new)
#overview(df)
```





## Thursday April 6th


#### Allow for hyperparameters in the neural network 
```{r}
nn_model <- nn_module(
  initialize = function(p, q1) {
    self$f <- nn_linear(p,q1)
    self$g <- nn_linear(q1,1)
    self$a <- nn_relu()
    self$s <- nn_sigmoid()
  },
  forward = function(x) {
    x %>%
      self$f() %>%
      self$a() %>%
      self$g() %>%
      self$s()
  }
)

# look at 10 row, 2 column tensor of random values
# changing second number --> change p in the function input
x <- torch_randn(10,2)
x

# p&q are hyperparameters
# will optimize the weights and biases of the neural network
nn_model(p = 2, q1 = 10)(x)
```

#### Luz Setup
```{r}
nn_model %>%
  setup(
    loss = nn_bce_loss(),
    optimizer = optim_adam
  )
```
*Setup function takes in 2 mandatory arguments --> the loss function and the optimizer (used for minmimizing loss)
  1. optimizer could be optim_adam, optim_rmsprop, ...



The code above is equivalent to...
```r
 F <- hh2_module()
 optimizer <- optim_adam(F$parameters, lr = 0.05)
 epochs <- 1000
      
 for (i in 1:epochs){
    loss <- nn_mse_loss()(F(X_tensor), y_tensor)
    optimizer$zero_grad()
    loss$backward()
    optimizer$step()
    }
```

Things we may want to specify

1. Epochs
1. Gradient descent step
1. x,y as tensors
1. Learning rate
1. what p & q are

#### Luz hyperparameters
```{r}
hh2_module <- nn_module(
  initialize = function(p, q1, q2, q3) {
    self$f <- nn_linear(p,q1)
    self$g <- nn_linear(q1,q2)
    self$h <- nn_linear(q2,q3)
    self$i <- nn_linear(q3,1)
    self$a <- nn_relu()
    self$s <- nn_sigmoid()
  },
  forward = function(x) {
    x %>%
      self$f() %>%
      self$a() %>%
      self$g() %>%
      self$a() %>%
      self$h() %>%
      self$a() %>%
      self$i() %>%
      self$s()
  }
)

hh2_module %>%
  setup(
    loss = nn_bce_loss(),
    optimizer = optim_adam
  ) %>%
  set_hparams(p=2, q1=5, q2=7,q3=5) %>%
  set_opt_hparams(lr=0.02)
```



#### Luz Fit

```{r}
fit_nn <- hh2_module %>%
  setup(
    loss = nn_bce_loss(),
    optimizer = optim_adam
  ) %>%
  set_hparams(p = 2, q1 = 5, q2 = 7, q3 = 5) %>%
  set_opt_hparams(lr = 0.02) %>%
  # Fit the neural network
  # Have to change formatting b/c torch can only read matrices, not data frames
  fit(
    data = list(
      as.matrix(df[,-1]),
      as.numeric(df[,1]) - 1
    ),
    epochs = 10,
    verbose = TRUE
  )
```



```r
# plots change in loss for the epochs specified
plot(fit_nn)
```


The output of the Luz allows you to use the familiar predict function
```r
predict(fit_nn, Xnew)
predict(fit_nn, cbind(rnorm(10), rnorm(10))) %>% as.array
```


#### Luz Validation Data

```{r}
test_ind <- sample(1:nrow(df), 23, replace = FALSE) 
```

ADD 'DF' CODE FROM PICTURE

```{r}
fit_nn <- hh2_module %>%
  setup(
    loss = nn_bce_loss(),
    optimizer = optim_adam
  ) %>%
  set_hparams(p = 2, q1 = 5, q2 = 7, q3 = 5) %>%
  set_opt_hparams(lr = 0.02) %>%
  fit(
    data = list(
      as.matrix(df[-test_ind,-1]),
      as.numeric(df[-test_ind,1]) - 1
    ),
    valid_data = list(
      as.matrix(df[+test_ind, -1]),
      as.numeric(df[+test_ind,1]) - 1
    ),
    epochs = 10,
    verbose = TRUE
  )
```

```r
plot(fit_nn)
```

* Luz has built in metrics (ex. accuracy, mse, ...)
* Do luz_metric_ and the function options will appear


```{r}
predicted <- torch_randn(sample(0:1, 100, replace = TRUE) )
expected <- torch_randn(sample(0:1, 100, replace = TRUE) )
metric <- luz_metric_binary_accuracy()

metric <-  metric$new()
metric$update(predicted, expected)
metric$compute()
```

```{r}
fit_nn <- hh2_module %>%
  setup(
    loss = nn_bce_loss(),
    optimizer = optim_adam,
    # specifying metrics we want to use
    metrics = list(
      luz_metric_binary_accuracy(),
      luz_metric_binary_auroc()
    )
  ) %>% 
  set_hparams(p = 2, q1 = 5, q2 = 7, q3 = 5) %>%
  set_opt_hparams(lr = 0.02) %>%
  fit(
    data = list(
      as.matrix(df[-test_ind,-1]),
      as.numeric(df[-test_ind,1]) - 1
    ),
    valid_data = list(
      as.matrix(df[+test_ind, -1]),
      as.numeric(df[+test_ind,1]) - 1
    ),
    epochs = 50,
    verbose = TRUE
  )
```

```{r}
plot(fit_nn)
```

